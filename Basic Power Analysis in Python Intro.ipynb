{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Power Analysis \n",
    "\n",
    "In the simplest terms, statistical power is the probability of detecting a ‘true’ effect when it exists. More specifically this may be thought of as estimating a statistically significant regression coefficient, or detecting a statistically significant difference between groups. For a given level of power, estimates of sample size required to detect the effect of interest can be made. These estimates can be made for a range of significance thresholds (not just p < .05)\n",
    "\n",
    "If we have a sample that is ‘not sufficiently powered’ it is possible that we could fail to find a relationship between treatment and outcome, even if one actually exists.  Equivalently, our estimated model coefficient may not be statistically significant when a true relationship exists.  Increasing sample size is the primary way to increase power in an experiment.  So the question becomes how large does ‘n’ have to be to have a sample sufficiently powered to detect the effect of a treatment on an outcome? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Role of Assumptions\n",
    "\n",
    "There are no simple formulas for determining sample size for all possible situations. Many research methodologies are so complex that power analysis is almost always impossible without a number of simplifying assumptions. A valid approach involves running a number of power and sample size analysis with a number of variations to accommodate differing assumptions related to effect size and modeling scenarios. When no analytical formulas exist for a given test or design, simulation may be necessary. Power analysis is essentially a data driven educated guess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An Analogy\n",
    "\n",
    "Jim Manzi, Author of Uncontrolled: The Surprising Payoff of Trial-and-Error for Business, Politics, and Society offers the following analogy in an Econ Talk podcast:\n",
    "\n",
    "*“Well, the power in a statistical experiment, and I often use this analogy, is sort of like the magnification power on the microscope you probably used in high school biology. It has on the side, 4x, 8x, 16x, which is how many times it can increase the apparent size of a physical object. And the metaphor I'd use is, if I try and use a child's microscope to carefully observe a section of a leaf looking for an insect that's a little smaller than an ant, and I don't observe the ant, I can reliably say: I don't see the insect, and therefore there is no bug there. If I use that exact same microscope to try and find on that exact same piece of leaf, not a bug but a tiny microbe that's, you know, smaller than a speck of dust, I'll look at it and I'll say: it's all kind of fuzzy, I see a lot of squiggly things; I think that little squiggle might be something or it might not. I don't see the microbe, but I can't reliably say that therefore there is no microbe there, because trying to zoom in closer and closer to look for something that small, all I see is a bunch of fuzz. So my failure to see the microbe is a statement about the precision of my instrument, not about whether there's really a microbe on the leaf.”*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Effect Size\n",
    "\n",
    "In Manzi’s analogy, he discusses the difference between looking for something as small as an insect, vs. something as small as a microbe. A 4x lens might be sufficiently powered to identify an ‘effect’ as small/large as an insect, but even a 32x lens might not be sufficiently powered to identify an ‘effect’ that is much smaller, such as a microbe.  \n",
    "\n",
    "How might we characterize effect size in a power analysis? Cohen (1988) is probably one of the most cited references in terms of work related to power and sample size analysis.\n",
    "\n",
    "Cohen established some rules of thumb for standard effect sizes (small, medium, large) as a guide for thinking about power analysis. Examples will be given below.\n",
    "\n",
    "\n",
    "## Four Key Elements in Power Analysis\n",
    "\n",
    "**1. sample size** - how many observations are required to 'power' our test. Do we have enough data to detect an effect if it exists for a given threshold level of statistical significance\n",
    "\n",
    "**2. effect size** - how large of an effect are we trying to detect? It is often helpful to think about, from a practical or business perspective, how large of impact does the test/treatment have to show to call it a success?\n",
    "\n",
    "**3. significance level** - denoted as 'alpha', the probability of rejecting the null hypothesis if it were true\n",
    "\n",
    "**4. power** - probability of rejecting the null hypotheses when it is false, or the probability of detecting an effect if it truly exists\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Required Sample Size for a Test\n",
    "\n",
    "We have said before that if we have a sample that is ‘not sufficiently powered’ it is possible that we could fail to find a relationship between treatment and outcome, even if one actually exists.  To avoid this, we need to know how large does ‘n’ have to be in order to detect a given effect at a given level of significance (often .05 or .10) with a probability of 80% (i.e. 80% power). Why 80% power?\n",
    "\n",
    "We have defined power generally as the probability of detecting a true effect when it exists. More formally power can be defined as (1-β), where β represents the probability of a type II error (not to be confused with a regression coefficient often represented by beta). A type two error is accepting a false null hypothesis, or saying there is no effect when one actually exists.  Therefore, (1-β), or power, represents the probability of rejecting a null hypothesis when it is false, or identifying an effect when it exists. This is what we want to be able to do.  Cohen (1992) suggests power analysis using  (1-β) = .80 because a) lower power implies higher β or probability of a type II error which may be unreasonable and b) higher power may require sample sizes that are too resource intensive.  Often .80 is used as a convention for general use (Cohen,1992)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Header | Second Header\n",
    "------------ | -------------\n",
    "Content cell 1 | Content cell 2\n",
    "Content column 1 | Content column 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Required Sample Size for a Difference in Means\n",
    "\n",
    "The following example comes from the UCLA Institute for Digital Research and Education (IDRE) statistical consulting website: https://stats.idre.ucla.edu/r/dae/power-analysis-for-two-group-independent-sample-t-test/\n",
    "\n",
    "\n",
    "*A clinical dietician wants to compare two different diets, A and B, for diabetic patients. She hypothesizes that diet A (Group 1) will be better than diet B (Group 2), in terms of lower blood glucose. She plans to get a random sample of diabetic patients and randomly assign them to one of the two diets. At the end of the experiment, which lasts 6 weeks, a fasting blood glucose test will be conducted on each patient. She also expects that the average difference in blood glucose measure between the two group will be about 10 mg/dl. Furthermore, she also assumes the standard deviation of blood glucose distribution for diet A to be 15 and the standard deviation for diet B to be 17. The dietician wants to know the number of subjects needed in each group assuming equal sized groups.*\n",
    "\n",
    "We should first note there were a lot of assumptions made there based on the investigators knowledge of the field. Sometimes that is helpful for setting up power and sample size calculations but often we may be doing something so new that we can't always make these assumptions easily so we may test a range of plausible assumptions. In the most ideal scenarios we may have historical baseline data to refer to to get estimates for differences in expected means and standard deviations that are also used to determine effect sizes used in the calculations that follow. \n",
    "\n",
    "But lets glean out some key metrics from the above scenario that will be helfpul for calculating the required sample size for each group in the test:\n",
    "\n",
    "Difference in Means: 10 (this is the difference in outcomes anticipated and is our basic effect size)\n",
    "Standard Deviation: 15 (diet A) 17 (diet B)\n",
    "\n",
    "\n",
    "\n",
    "## Syntax and Parameters  tt_ind_solve from statsmodels in Python\n",
    "\n",
    "For our power and sample size calculations we will use the tt_ind_solve_power function from statsmodels:\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.stats.power.tt_ind_solve_power.html \n",
    "\n",
    "**Syntax:** *statsmodels.stats.power.tt_ind_solve_power(effect_size=None, nobs1=None, alpha=None, power=None, ratio=1.0, alternative='two-sided')*\n",
    "\n",
    "Parameter Definitions:\n",
    "\n",
    "**effect_size:** this standardized effect size which is the expected difference between the two means divided by the standard deviation. effect_size has to be positive.\n",
    "\n",
    "d = $\\frac{| \\mu_{1} - \\mu_{2} |}{\\sigma}$ (standardized effect size)\n",
    "\n",
    "Cohen devised some rules of thumb for conventional effect sizes including small, medium, and large for a number of tests:\n",
    "\n",
    "Type of Test | Small | Medium | Large\n",
    "------------ | -------------|------------ | -------------\n",
    "tests for means | 0.2 | 0.5 | 0.8\n",
    "tests for proportions | 0.2 | 0.5 | 0.8\n",
    "chi-square | 0.1 | 0.3 | 0.5\n",
    "correlations | 0.1 | 0.3 | 0.5\n",
    "analysis of variance | 0.1 | 0.25 | 0.4\n",
    "general linear model | 0.02 | 0.15 | 0.35\n",
    "\n",
    "The effect sizes that really matter are those most relevant to the problem and subject matter regardless of how they are classified above. However,these guidlines may be useful in situations where we don't really know a lot about the data and we want very rough back of the envelope idea of sample size requirements in absence in more data.\n",
    "\n",
    "**nobs1:** number of observations of sample 1. The number of observations of sample two is ratio times the size of sample 1, i.e. nobs2 = nobs1 * ratio\n",
    "\n",
    "**alpha:** specifiec level of significance \n",
    "\n",
    "**power:** specified level of power \n",
    "\n",
    "**ratio:** this is the ratio of the number of observations in sample 2 relative to sample 1. \n",
    "\n",
    "**alternative:** options include ‘two-sided’ (default), ‘larger’, ‘smaller’ indicting if calculated for a two-sided (default) or one sided test. The one-sided test can be either ‘larger’, ‘smaller’.\n",
    "\n",
    "\n",
    "Based on the example above, we can calculate the required sample size specifying the parameters and using the syntax above as follows:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats import power\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.0312195418814\n",
      "0.6237828615518053\n"
     ]
    }
   ],
   "source": [
    "# calculate effect size 'd'\n",
    "sigmaA = 15 # standard deviation for group A\n",
    "sigmaB = 17 # standard deviation for group B\n",
    "Q0 = .5 # proportion in the control group (.5 implies equal sized groups)\n",
    "\n",
    "sigma_pooled = np.sqrt((np.square(sigmaA)*(1/Q0 -1 ) + np.square(sigmaB)*(1/(1-Q0) -1 ) )/((1/Q0 -1 ) + (1/(1-Q0) -1 ))) # pooled standard deviation\n",
    "\n",
    "diff = 10 # this is the expected difference in means we are testing (mu1 - mu2)\n",
    "\n",
    "d = abs(diff)/sigma_pooled\n",
    "\n",
    "print(sigma_pooled)\n",
    "\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to Cohoen's conventional effect sizes for a difference in means, d = .62 would be considered somewhere between a medium or large effect size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41.32581080152109"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calculate required sample size (assuming equal sized groups i.e. ration = 1, 80% power and 5% significance)\n",
    "statsmodels.stats.power.tt_ind_solve_power(effect_size= d, nobs1=None, alpha=.05, power= .8, ratio=1.0, alternative='two-sided')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above result indicates that we would need about 41 subjects in each group to run this test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating Minimum Effect Sizes\n",
    "\n",
    "Sometimes we might want to know, given a certain sample size at hand, what is the smallest effect size we could detect? Lets say we have a sample size of 41, what is the smallest effect size we could expect to detect if we run this test based on the same assumptions above? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.626318447155647"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power.tt_ind_solve_power(alpha =0.05, power=0.8, nobs1=41)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This makes sense because we powered our original test to be able to detect an effect size = .62. Suppose we attempted to run our test but budget or time constraints resulted in having a reduced sample size = 25 in each group. What is the smallest effect size we could detect if we run our test?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8087077886680407"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "power.tt_ind_solve_power(alpha =0.05, power=0.8, nobs1=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice, with less data, we are no longer powered to detect an effect size of .626. (you can think of this as 62% of the standard deviation or an observed difference between groups ~ .63*16 = 10) If we only have 25 subjects in each group, our observed difference has to be larger to detect it with 80% power and a 5% level of significance. (~ .8*16 = 12.8 vs. 10). In certain applications this can make a big difference in the conclusions we are able to make."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above example was based on power and sample size calculations for a difference in means based on a normally distributed outcome. For non-normal outcomes (like count variables following a negative binomial or poisson distribution) or extremely skewed outcomes (like gamma distributed variables) different approaches may be required. See references below for further explanations.\n",
    "\n",
    "## References\n",
    "\n",
    "A Power Primer. Jacob Cowen. Quantitative Methods in Psychology. Psychological Bulletin, 1992, Vol.1 155-159.\n",
    "\n",
    "Cohen J. Statistical Power Analysis for the Behavioral Sciences 2nd ed. Hillsdale, NJ:\n",
    "Lawrence Erlbaum; 1988.\n",
    "\n",
    "MacCallum, R. C., Browne, M. W., & Sugawara, H. M. (1996). Power analysis and determination of sample size for covariance structure modeling. Psychological Methods, 1, 130-149.\n",
    "\n",
    "Everitt, S. (1975). Multivariate analysis: The need for data, and other problems. British Journal of Psychiatry, 126, 237-240.\n",
    "\n",
    "https://www.statsmodels.org/stable/generated/statsmodels.stats.power.tt_ind_solve_power.html \n",
    "\n",
    "Cundill, Bonnie & Alexander, Neal. (2015). Sample size calculations for skewed distributions. \n",
    "BMC medical research methodology. 15. 28. 10.1186/s12874-015-0023-0.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
